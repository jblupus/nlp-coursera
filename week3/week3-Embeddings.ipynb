{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find duplicate questions on StackOverflow by their embeddings\n",
    "\n",
    "In this assignment you will learn how to calculate a similarity for pieces of text. Using this approach you will know how to find duplicate questions from [StackOverflow](https://stackoverflow.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "\n",
    "In this task you will you will need the following libraries:\n",
    "- [StarSpace](https://github.com/facebookresearch/StarSpace) — a general-purpose model for efficient learning of entity embeddings from Facebook\n",
    "- [Gensim](https://radimrehurek.com/gensim/) — a tool for solving various NLP-related tasks (topic modeling, text representation, ...)\n",
    "- [Numpy](http://www.numpy.org) — a package for scientific computing.\n",
    "- [scikit-learn](http://scikit-learn.org/stable/index.html) — a tool for data mining and data analysis.\n",
    "- [Nltk](http://www.nltk.org) — a platform to work with human language data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The following cell will download all data required for this assignment into the folder `week3/data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from common.download_utils import download_week3_resources\n",
    "\n",
    "#download_week3_resources() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading\n",
    "We will create a grader instace below and use it to collect your answers. Note that these outputs will be stored locally inside grader and will be uploaded to platform only after running submiting function in the last part of this assignment. If you want to make partial submission, you can run that cell any time you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grader import Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader = Grader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding\n",
    "\n",
    "To solve the problem, you will use two different models of embeddings:\n",
    "\n",
    " - [Pre-trained word vectors](https://code.google.com/archive/p/word2vec/) from Google which were trained on a part of Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. `GoogleNews-vectors-negative300.bin.gz` will be downloaded in `download_week3_resources()`.\n",
    " - Representations using StarSpace on StackOverflow data sample. You will need to train them from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always easier to start with pre-trained embeddings. Unpack the pre-trained Goggle's vectors and upload them using the function [KeyedVectors.load_word2vec_format](https://radimrehurek.com/gensim/models/keyedvectors.html) from gensim library with the parameter *binary=True*. If the size of the embeddings is larger than the avaliable memory, you could load only a part of the embeddings by defining the parameter *limit* (recommended: 500000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_embeddings = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to work with Google's word2vec embeddings?\n",
    "\n",
    "Once you have loaded the representations, make sure you can access them. First, you can check if the loaded embeddings contain a word:\n",
    "    \n",
    "    'word' in wv_embeddings\n",
    "    \n",
    "Second, to get the corresponding embedding you can use the square brackets:\n",
    "\n",
    "    wv_embeddings['word']\n",
    " \n",
    "### Checking that the embeddings are correct \n",
    " \n",
    "To prevent any errors during the first stage, we can check that the loaded embeddings are correct. You can call the function *check_embeddings*, implemented below, which runs 3 tests:\n",
    "1. Find the most similar word for provided \"positive\" and \"negative\" words.\n",
    "2. Find which word from the given list doesn’t go with the others.\n",
    "3. Find the most similar word for the provided one.\n",
    "\n",
    "In the right case the function will return the string *These embeddings look good*. Othervise, you need to validate the previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_embeddings(embeddings):\n",
    "    error_text = \"Something wrong with your embeddings ('%s test isn't correct).\"\n",
    "    most_similar = embeddings.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "    if len(most_similar) < 1 or most_similar[0][0] != 'queen':\n",
    "        return error_text % \"Most similar\"\n",
    "\n",
    "    doesnt_match = embeddings.doesnt_match(['breakfast', 'cereal', 'dinner', 'lunch'])\n",
    "    if doesnt_match != 'cereal':\n",
    "        return error_text % \"Doesn't match\"\n",
    "    \n",
    "    most_similar_to_given = embeddings.most_similar_to_given('music', ['water', 'sound', 'backpack', 'mouse'])\n",
    "    if most_similar_to_given != 'sound':\n",
    "        return error_text % \"Most similar to given\"\n",
    "    \n",
    "    return \"These embeddings look good.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These embeddings look good.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joao/Documents/Coursera/natural-language-processing/venv/lib/python3.7/site-packages/gensim/models/keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    }
   ],
   "source": [
    "print(check_embeddings(wv_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From word to text embeddings\n",
    "\n",
    "**Task 1 (Question2Vec).** Usually, we have word-based embeddings, but for the task we need to create a representation for the whole question. It could be done in different ways. In our case we will use a **mean** of all word vectors in the question. Now you need to implement the function *question_to_vec*, which calculates the question representation described above. This function should work with the input text as is without any preprocessing.\n",
    "\n",
    "Note that there could be words without the corresponding embeddings. In this case, you can just skip these words and don't take them into account during calculating the result. If the question doesn't contain any known word with embedding, the function should return a zero vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_vec(question, embeddings, dim):\n",
    "    \"\"\"\n",
    "        question: a string\n",
    "        embeddings: dict where the key is a word and a value is its' embedding\n",
    "        dim: size of the representation\n",
    "\n",
    "        result: vector representation for the question\n",
    "    \"\"\"\n",
    "    question_vec = np.zeros(dim)\n",
    "    count = 0\n",
    "    for word in question.split():\n",
    "        if word not in embeddings:\n",
    "            continue\n",
    "        question_vec += embeddings[word]\n",
    "        count += 1\n",
    "    if count != 0:\n",
    "        question_vec /= count\n",
    "    return question_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the basic correctness of your implementation, run the function *question_to_vec_tests*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_vec_tests():\n",
    "    if (np.zeros(300) != question_to_vec('', wv_embeddings)).any():\n",
    "        return \"You need to return zero vector for empty question.\"\n",
    "    if (np.zeros(300) != question_to_vec('thereisnosuchword', wv_embeddings)).any():\n",
    "        return \"You need to return zero vector for the question, which consists only unknown words.\"\n",
    "    if (wv_embeddings['word'] != question_to_vec('word', wv_embeddings)).any():\n",
    "        return \"You need to check the corectness of your function.\"\n",
    "    if ((wv_embeddings['I'] + wv_embeddings['am']) / 2 != question_to_vec('I am', wv_embeddings)).any():\n",
    "        return \"Your function should calculate a mean of word vectors.\"\n",
    "    if (wv_embeddings['word'] != question_to_vec('thereisnosuchword word', wv_embeddings)).any():\n",
    "        return \"You should not consider words which embeddings are unknown.\"\n",
    "    return \"Basic tests are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tests are passed.\n"
     ]
    }
   ],
   "source": [
    "print(question_to_vec_tests())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can submit embeddings for the questions from the file *test_embeddings.tsv* to earn the points. In this task you don't need to transform the text of a question somehow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/joao/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from util import array_to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task Question2Vec is: 0.019293891059027776\n",
      "-0.028727213541666668\n",
      "0.046056111653645836\n",
      "0.08525933159722222\n",
      "0.02430555555555...\n"
     ]
    }
   ],
   "source": [
    "question2vec_result = []\n",
    "for question in open('data/test_embeddings.tsv'):\n",
    "    question = question.strip()\n",
    "    answer = question_to_vec(question, wv_embeddings)\n",
    "    question2vec_result = np.append(question2vec_result, answer)\n",
    "\n",
    "grader.submit_tag('Question2Vec', array_to_string(question2vec_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a method to create a representation of any sentence and we are ready for the first evaluation. So, let's check how well our solution (Google's vectors + *question_to_vec*) will work.\n",
    "\n",
    "## Evaluation of text similarity\n",
    "\n",
    "We can imagine that if we use good embeddings, the cosine similarity between the duplicate sentences should be less than for the random ones. Overall, for each pair of duplicate sentences we can generate *R* random negative examples and find out the position of the correct duplicate.  \n",
    "\n",
    "For example, we have the question *\"Exceptions What really happens\"* and we are sure that another question *\"How does the catch keyword determine the type of exception that was thrown\"* is a duplicate. But our model doesn't know it and tries to find out the best option also among questions like *\"How Can I Make These Links Rotate in PHP\"*, *\"NSLog array description not memory address\"* and *\"PECL_HTTP not recognised php ubuntu\"*. The goal of the model is to rank all these 4 questions (1 *positive* and *R* = 3 *negative*) in the way that the correct one is in the first place.\n",
    "\n",
    "However, it is unnatural to count on that the best candidate will be always in the first place. So let us consider the place of the best candidate in the sorted list of candidates and formulate a metric based on it. We can fix some *K* — a reasonalble number of top-ranked elements and *N* — a number of queries (size of the sample).\n",
    "\n",
    "### Hits@K\n",
    "\n",
    "The first simple metric will be a number of correct hits for some *K*:\n",
    "$$ \\text{Hits@K} = \\frac{1}{N}\\sum_{i=1}^N \\, [dup_i \\in topK(q_i)]$$\n",
    "\n",
    "where $q_i$ is the i-th query, $dup_i$ is its duplicate, $topK(q_i)$ is the top K elements of the ranked sentences provided by our model and the operation $[dup_i \\in topK(q_i)]$ equals 1 if the condition is true and 0 otherwise (more details about this operation could be found [here](https://en.wikipedia.org/wiki/Iverson_bracket)).\n",
    "\n",
    "\n",
    "### DCG@K\n",
    "The second one is a simplified [DCG metric](https://en.wikipedia.org/wiki/Discounted_cumulative_gain):\n",
    "\n",
    "$$ \\text{DCG@K} = \\frac{1}{N} \\sum_{i=1}^N\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le K] $$\n",
    "\n",
    "where $rank_{dup_i}$ is a position of the duplicate in the sorted list of the nearest sentences for the query $q_i$. According to this metric, the model gets a higher reward for a higher position of the correct answer. If the answer does not appear in topK at all, the reward is zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation examples\n",
    "\n",
    "Let's calculate the described metrics for the toy example introduced above. In this case $N$ = 1 and the correct candidate for $q_1$ is *\"How does the catch keyword determine the type of exception that was thrown\"*. Consider the following ranking of the candidates:\n",
    "1. *\"How Can I Make These Links Rotate in PHP\"*\n",
    "2. *\"How does the catch keyword determine the type of exception that was thrown\"*\n",
    "3. *\"NSLog array description not memory address\"*\n",
    "4. *\"PECL_HTTP not recognised php ubuntu\"*\n",
    "\n",
    "Using the ranking above, calculate *Hits@K* metric for *K = 1, 2, 4*: \n",
    " \n",
    "- [K = 1] $\\text{Hits@1} = \\frac{1}{1}\\sum_{i=1}^1 \\, [dup_i \\in top1(q_i)] = [dup_1 \\in top1(q_1)] = 0$ because the correct answer doesn't appear in the *top1* list.\n",
    "- [K = 2] $\\text{Hits@2} = \\frac{1}{1}\\sum_{i=1}^1 \\, [dup_i \\in top2(q_i)] = [dup_1 \\in top2(q_1)] = 1$ because $rank_{dup_1} = 2$.\n",
    "- [K = 4] $\\text{Hits@4} = \\frac{1}{1}\\sum_{i=1}^1 \\, [dup_i \\in top4(q_i)] = [dup_1 \\in top4(q_1)] = 1$\n",
    "\n",
    "Using the ranking above, calculate *DCG@K* metric for *K = 1, 2, 4*:\n",
    "\n",
    "- [K = 1] $\\text{DCG@1} = \\frac{1}{1} \\sum_{i=1}^1\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 1] = \\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 1] = 0$ because the correct answer doesn't appear in the top1 list.\n",
    "- [K = 2] $\\text{DCG@2} = \\frac{1}{1} \\sum_{i=1}^1\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 2] = \\frac{1}{\\log_2{3}}$, because $rank_{dup_1} = 2$.\n",
    "- [K = 4] $\\text{DCG@4} = \\frac{1}{1} \\sum_{i=1}^1\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 4] = \\frac{1}{\\log_2{3}}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks 2 and 3 (HitsCount and DCGScore).** Implement the functions *hits_count* and *dcg_score* as described above. Each function has two arguments: *dup_ranks* and *k*. *dup_ranks* is a list which contains *values of ranks* of duplicates. For example, *dup_ranks* is *[2]* for the example provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hits_count(dup_ranks, k):\n",
    "    \"\"\"\n",
    "        dup_ranks: list of duplicates' ranks; one rank per question; \n",
    "                   length is a number of questions which we are looking for duplicates; \n",
    "                   rank is a number from 1 to len(candidates of the question); \n",
    "                   e.g. [2, 3] means that the first duplicate has the rank 2, the second one — 3.\n",
    "        k: number of top-ranked elements (k in Hits@k metric)\n",
    "\n",
    "        result: return Hits@k value for current ranking\n",
    "    \"\"\"\n",
    "    in_top = lambda dup: 1 if dup <= k else 0\n",
    "    return np.sum([in_top(dup) for dup in dup_ranks])/len(dup_ranks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code on the tiny examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hits():\n",
    "    # *Evaluation example*\n",
    "    # answers — dup_i\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\"]\n",
    "    \n",
    "    # candidates_ranking — the ranked sentences provided by our model\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"NSLog array description not memory address\",\n",
    "                           \"PECL_HTTP not recognised php ubuntu\"]]\n",
    "    # dup_ranks — position of the dup_i in the list of ranks +1\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    \n",
    "    # correct_answers — the expected values of the result for each k from 1 to 4\n",
    "    correct_answers = [0, 1, 1, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function.\"\n",
    "    \n",
    "    # Other tests\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\", \n",
    "               \"Convert Google results object (pure js) to Python object\"]\n",
    "    \n",
    "    # The first test: both duplicates on the first position in ranked list\n",
    "    candidates_ranking = [[\"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"How Can I Make These Links Rotate in PHP\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [1, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both duplicates on the first position in ranked list).\"\n",
    "        \n",
    "    # The second test: one candidate on the first position, another — on the second\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0.5, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: one candidate on the first position, another — on the second).\"\n",
    "\n",
    "    # The third test: both candidates on the second position\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"WPF- How to update the changes in list item of a list\",\n",
    "                           \"Convert Google results object (pure js) to Python object\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both candidates on the second position).\"\n",
    "\n",
    "    return \"Basic test are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic test are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_hits())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_score(dup_ranks, k):\n",
    "    \"\"\"\n",
    "        dup_ranks: list of duplicates' ranks; one rank per question; \n",
    "                   length is a number of questions which we are looking for duplicates; \n",
    "                   rank is a number from 1 to len(candidates of the question); \n",
    "                   e.g. [2, 3] means that the first duplicate has the rank 2, the second one — 3.\n",
    "        k: number of top-ranked elements (k in DCG@k metric)\n",
    "\n",
    "        result: return DCG@k value for current ranking\n",
    "    \"\"\"\n",
    "    in_top = lambda dup: 1 if dup <= k else 0\n",
    "    log_rank = lambda rank: 1/np.log2(1 + rank) if in_top(rank) else 0\n",
    "    return np.sum([log_rank(dup_rank) for dup_rank in dup_ranks])/len(dup_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dcg():\n",
    "    # *Evaluation example*\n",
    "    # answers — dup_i\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\"]\n",
    "    \n",
    "    # candidates_ranking — the ranked sentences provided by our model\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"NSLog array description not memory address\",\n",
    "                           \"PECL_HTTP not recognised php ubuntu\"]]\n",
    "    # dup_ranks — position of the dup_i in the list of ranks +1\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    \n",
    "    # correct_answers — the expected values of the result for each k from 1 to 4\n",
    "    correct_answers = [0, 1 / (np.log2(3)), 1 / (np.log2(3)), 1 / (np.log2(3))]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function.\"\n",
    "    \n",
    "    # Other tests\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\", \n",
    "               \"Convert Google results object (pure js) to Python object\"]\n",
    "\n",
    "    # The first test: both duplicates on the first position in ranked list\n",
    "    candidates_ranking = [[\"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"How Can I Make These Links Rotate in PHP\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [1, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both duplicates on the first position in ranked list).\"\n",
    "        \n",
    "    # The second test: one candidate on the first position, another — on the second\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0.5, (1 + (1 / (np.log2(3)))) / 2]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: one candidate on the first position, another — on the second).\"\n",
    "        \n",
    "    # The third test: both candidates on the second position\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\",\n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"WPF- How to update the changes in list item of a list\",\n",
    "                           \"Convert Google results object (pure js) to Python object\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0, 1 / (np.log2(3))]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both candidates on the second position).\"\n",
    "\n",
    "    return \"Basic test are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic test are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_dcg())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit results of the functions *hits_count* and *dcg_score* for the following examples to earn the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = [\n",
    "    [1],\n",
    "    [1, 2],\n",
    "    [2, 1],\n",
    "    [1, 2, 3],\n",
    "    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    [9, 5, 4, 2, 8, 10, 7, 6, 1, 3],\n",
    "    [4, 3, 5, 1, 9, 10, 7, 8, 2, 6],\n",
    "    [5, 1, 7, 6, 2, 3, 8, 9, 10, 4],\n",
    "    [6, 3, 1, 4, 7, 2, 9, 8, 10, 5],\n",
    "    [10, 9, 8, 7, 6, 5, 4, 3, 2, 1],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task HitsCount is: 1.0\n",
      "0.5\n",
      "1.0\n",
      "0.5\n",
      "1.0\n",
      "0.3333333333333333\n",
      "0.6666666666666666\n",
      "1.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1....\n"
     ]
    }
   ],
   "source": [
    "hits_results = []\n",
    "for example in test_examples:\n",
    "    for k in range(len(example)):\n",
    "        hits_results.append(hits_count(example, k + 1))\n",
    "grader.submit_tag('HitsCount', array_to_string(hits_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task DCGScore is: 1.0\n",
      "0.5\n",
      "0.8154648767857288\n",
      "0.5\n",
      "0.8154648767857288\n",
      "0.3333333333333333\n",
      "0.5436432511904858\n",
      "0.7103099178...\n"
     ]
    }
   ],
   "source": [
    "dcg_results = []\n",
    "for example in test_examples:\n",
    "    for k in range(len(example)):\n",
    "        dcg_results.append(dcg_score(example, k + 1))\n",
    "grader.submit_tag('DCGScore', array_to_string(dcg_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  First solution: pre-trained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with predefined train, validation and test corpora. All the files are tab-separated, but have a different format:\n",
    " - *train* corpus contains similar sentences at the same row.\n",
    " - *validation* corpus contains the following columns: *question*, *similar question*, *negative example 1*, *negative example 2*, ... \n",
    " - *test* corpus contains the following columns: *question*, *example 1*, *example 2*, ...\n",
    "\n",
    "Validation corpus will be used for the intermediate validation of models. The test data will be necessary for submitting the quality of your model in the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should read *validation* corpus, located at `data/validation.tsv`. You will use it later to evaluate current solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(filename):\n",
    "    data = []\n",
    "    for line in open(filename, encoding='utf-8'):\n",
    "        data.append(line.strip().split('\\t'))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = read_corpus('data/validation.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use cosine distance to rank candidate questions which you need to implement in the function *rank_candidates*. The function should return a sorted list of pairs *(initial position in candidates list, candidate)*. Index of some pair corresponds to its rank (the first is the best). For example, if the list of candidates was *[a, b, c]* and the most similar is *c*, then *a* and *b*, the function should return a list *[(2, c), (0, a), (1, b)]*.\n",
    "\n",
    "Pay attention, if you use the function *cosine_similarity* from *sklearn.metrics.pairwise* to calculate similarity because it works in a different way: most similar objects has greatest similarity. It's preferable to use a vectorized version of *cosine_similarity* function. Try to compute similarity at once and not use list comprehension. It should speed up your computations significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_candidates(question, candidates, embeddings, dim):\n",
    "    \"\"\"\n",
    "        question: a string\n",
    "        candidates: a list of strings (candidates) which we want to rank\n",
    "        embeddings: some embeddings\n",
    "        dim: dimension of the current embeddings\n",
    "        \n",
    "        result: a list of pairs (initial position in the list, question)\n",
    "    \"\"\"\n",
    "    \n",
    "    sentence_to_vec = lambda sentence: question_to_vec(sentence, embeddings, dim)\n",
    "    sentences_to_vec = lambda sentences: [sentence_to_vec(sentence) for sentence in sentences]\n",
    "    cosine_sim = lambda qs, vec: cosine_similarity(qs, vec)\n",
    "    question_vec = np.array(sentence_to_vec(question)).reshape(1, dim)\n",
    "    candidates_vec = sentences_to_vec(candidates)\n",
    "    \n",
    "    canditades_sim = cosine_sim(question_vec, candidates_vec)\n",
    "    \n",
    "    initial_position = [(i, e) for i,e in enumerate(candidates)]\n",
    "    candidates_sim = dict(zip(initial_position, canditades_sim[0]))\n",
    "    candidates_sim = sorted(candidates_sim.items(), key = lambda kv:(kv[1], kv[0]), reverse=True)\n",
    "    candidates_sim = [ cand for cand,_ in candidates_sim]\n",
    "\n",
    "    return candidates_sim\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code on the tiny examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rank_candidates():\n",
    "    questions = ['converting string to list', 'Sending array via Ajax fails']\n",
    "    candidates = [['Convert Google results object (pure js) to Python object', \n",
    "                   'C# create cookie from string and send it',\n",
    "                   'How to use jQuery AJAX for an outside domain?'], \n",
    "                  ['Getting all list items of an unordered list in PHP', \n",
    "                   'WPF- How to update the changes in list item of a list', \n",
    "                   'select2 not displaying search results']]\n",
    "    results = [[(1, 'C# create cookie from string and send it'), \n",
    "                (0, 'Convert Google results object (pure js) to Python object'), \n",
    "                (2, 'How to use jQuery AJAX for an outside domain?')],\n",
    "               [(0, 'Getting all list items of an unordered list in PHP'), \n",
    "                (2, 'select2 not displaying search results'), \n",
    "                (1, 'WPF- How to update the changes in list item of a list')]]\n",
    "    for question, q_candidates, result in zip(questions, candidates, results):\n",
    "        ranks = rank_candidates(question, q_candidates, wv_embeddings, 300)\n",
    "        if not np.all(ranks == result):\n",
    "            return \"Check the function.\"\n",
    "    return \"Basic tests are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tests are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_rank_candidates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test the quality of the current approach. Run the next two cells to get the results. Pay attention that calculation of similarity between vectors takes time and this calculation is computed approximately in 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-88a145abc173>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mranks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrank_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwv_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mwv_ranking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mranks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-104-69dcec6448fa>\u001b[0m in \u001b[0;36mrank_candidates\u001b[0;34m(question, candidates, embeddings, dim)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mcosine_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mquestion_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_to_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mcandidates_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences_to_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mcanditades_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidates_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-104-69dcec6448fa>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(sentences)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0msentence_to_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquestion_to_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0msentences_to_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msentence_to_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mcosine_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mquestion_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_to_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-104-69dcec6448fa>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0msentence_to_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquestion_to_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0msentences_to_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msentence_to_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mcosine_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mquestion_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_to_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-104-69dcec6448fa>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \"\"\"\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0msentence_to_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquestion_to_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0msentences_to_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msentence_to_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mcosine_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-102-2e1f4e6b21e1>\u001b[0m in \u001b[0;36mquestion_to_vec\u001b[0;34m(question, embeddings, dim)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mquestion_vec\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Coursera/natural-language-processing/venv/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, entities)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Coursera/natural-language-processing/venv/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Coursera/natural-language-processing/venv/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    461\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_norm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetflags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "wv_ranking = []\n",
    "for index, line in enumerate(validation):\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings, 300)\n",
    "    wv_ranking.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(wv_ranking, k), k, hits_count(wv_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did all the steps correctly, you should be frustrated by the received results. Let's try to understand why the quality is so low. First of all, when you work with some data it is necessary to have an idea how the data looks like. Print several questions from the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in validation[:3]:\n",
    "    q, *examples = line\n",
    "    print(q, *examples[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we deal with the raw data. It means that we have many punctuation marks, special characters and unlowercased letters. In our case, it could lead to the situation where we can't find some embeddings, e.g. for the word \"grid?\". \n",
    "\n",
    "To solve this problem you should use the functions *text_prepare* from the previous assignments to prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import text_prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now transform all the questions from the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_validation = []\n",
    "for line in validation:\n",
    "    prepared_validation.append([text_prepare(text) for text in line])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the approach again after the preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_prepared_ranking = []\n",
    "for line in prepared_validation:\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings)\n",
    "    wv_prepared_ranking.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.306 | Hits@   1: 0.306\n",
      "DCG@   5: 0.379 | Hits@   5: 0.444\n",
      "DCG@  10: 0.396 | Hits@  10: 0.496\n",
      "DCG@ 100: 0.430 | Hits@ 100: 0.665\n",
      "DCG@ 500: 0.453 | Hits@ 500: 0.841\n",
      "DCG@1000: 0.469 | Hits@1000: 1.000\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(wv_prepared_ranking, k), \n",
    "                                              k, hits_count(wv_prepared_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, prepare also train and test data, because you will need it in the future:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_file(in_, out_):\n",
    "    out = open(out_, 'w')\n",
    "    for line in open(in_, encoding='utf8'):\n",
    "        line = line.strip().split('\\t')\n",
    "        new_line = [text_prepare(q) for q in line]\n",
    "        print(*new_line, sep='\\t', file=out)\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_files = ['data/train.tsv', 'data/test.tsv','data/validation.tsv']\n",
    "out_files = ['data/prepared_train.tsv', 'data/prepared_test.tsv','data/prepared_validation.tsv']\n",
    "files = zip(in_files, out_files)\n",
    "for in_, out_ in files:\n",
    "    prepare_file(in_, out_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4 (W2VTokenizedRanks).** For each question from prepared *test.tsv* submit the ranks of the candidates to earn the points. The calculations should take about 3-5 minutes. Pay attention that the function *rank_candidates* returns a ranking, while in this case you should find a position in this ranking. Ranks should start with 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import matrix_to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-059b5ecf9371>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepared_test_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mranks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrank_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwv_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mranked_candidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mranks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mw2v_ranks_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mranked_candidates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mranked_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-93-230dc3df4d5f>\u001b[0m in \u001b[0;36mrank_candidates\u001b[0;34m(question, candidates, embeddings, dim)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mcosine_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mquestion_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_to_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mcandidates_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences_to_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mcanditades_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidates_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-93-230dc3df4d5f>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(sentences)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0msentence_to_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquestion_to_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwv_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0msentences_to_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msentence_to_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mcosine_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mquestion_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_to_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-93-230dc3df4d5f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0msentence_to_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquestion_to_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwv_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0msentences_to_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msentence_to_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mcosine_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mquestion_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_to_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-93-230dc3df4d5f>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \"\"\"\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0msentence_to_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquestion_to_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwv_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0msentences_to_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msentence_to_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mcosine_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-97bd7ae5b467>\u001b[0m in \u001b[0;36mquestion_to_vec\u001b[0;34m(question, embeddings, dim)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mquestion_vec\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Coursera/natural-language-processing/venv/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, entities)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Coursera/natural-language-processing/venv/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Coursera/natural-language-processing/venv/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    463\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetflags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "w2v_ranks_results = []\n",
    "prepared_test_data = 'data/prepared_test.tsv'\n",
    "for line in open(prepared_test_data):\n",
    "    q, *ex = line.strip().split('\\t')\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings, 300)\n",
    "    ranked_candidates = [r[0] for r in ranks]\n",
    "    w2v_ranks_results.append([ranked_candidates.index(i) + 1 for i in range(len(ranked_candidates))])\n",
    "    \n",
    "grader.submit_tag('W2VTokenizedRanks', matrix_to_string(w2v_ranks_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced solution: StarSpace embeddings\n",
    "\n",
    "Now you are ready to train your own word embeddings! In particular, you need to train embeddings specially for our task of duplicates detection. Unfortunately, StarSpace cannot be run on Windows and we recommend to use provided\n",
    "[docker container](https://github.com/hse-aml/natural-language-processing/blob/master/Docker-tutorial.md) or other alternatives. Don't delete results of this task because you will need it in the final project.\n",
    "\n",
    "### How it works and what's the main difference with word2vec?\n",
    "The main point in this section is that StarSpace can be trained specifically for some tasks. In contrast to word2vec model, which tries to train similar embeddings for words in similar contexts, StarSpace uses embeddings for the whole sentence (just as a sum of embeddings of words and phrases). Despite the fact that in both cases we get word embeddings as a result of the training, StarSpace embeddings are trained using some supervised data, e.g. a set of similar sentence pairs, and thus they can better suit the task.\n",
    "\n",
    "In our case, StarSpace should use two types of sentence pairs for training: \"positive\" and \"negative\". \"Positive\" examples are extracted from the train sample (duplicates, high similarity) and the \"negative\" examples are generated randomly (low similarity assumed). \n",
    "\n",
    "### How to choose the best params for the model?\n",
    "Normally, you would start with some default choice and then run extensive experiments to compare different strategies. However, we have some recommendations ready for you to save your time:\n",
    "- Be careful with choosing the suitable training mode. In this task we want to explore texts similarity which corresponds to *trainMode = 3*.\n",
    "- Use adagrad optimization (parameter *adagrad = true*).\n",
    "- Set the length of phrase equal to 1 (parameter *ngrams*), because we need embeddings only for words.\n",
    "- Don't use a large number of *epochs* (we think that 5 should be enough).\n",
    "- Try dimension *dim* equal to 100.\n",
    "- To compare embeddings usually *cosine* *similarity* is used.\n",
    "- Set *minCount* greater than 1 (for example, 2) if you don't want to get embeddings for extremely rare words.\n",
    "- Parameter *verbose = true* could show you the progress of the training process.\n",
    "- Set parameter *fileFormat* equals *labelDoc*.\n",
    "- Parameter *negSearchLimit* is responsible for a number of negative examples which is used during the training. We think that 10 will be enought for this task.\n",
    "- To increase a speed of training we recommend to set *learning rate* to 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train StarSpace embeddings for unigrams on the train dataset. You don't need to change the format of the input data. Just don't forget to use prepared version of the training data. \n",
    "\n",
    "If you follow the instruction, the training process will take about 1 hour. The size of the embeddings' dictionary should be approximately 100 000 (number of lines in the result file). If you got significantly more than this number, try to check all the instructions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments: \n",
      "lr: 0.05\n",
      "dim: 100\n",
      "epoch: 5\n",
      "maxTrainTime: 8640000\n",
      "validationPatience: 10\n",
      "saveEveryEpoch: 0\n",
      "loss: hinge\n",
      "margin: 0.05\n",
      "similarity: cosine\n",
      "maxNegSamples: 10\n",
      "negSearchLimit: 10\n",
      "batchSize: 5\n",
      "thread: 10\n",
      "minCount: 2\n",
      "minCountLabel: 1\n",
      "label: __label__\n",
      "label: __label__\n",
      "ngrams: 1\n",
      "bucket: 2000000\n",
      "adagrad: 1\n",
      "trainMode: 3\n",
      "fileFormat: labelDoc\n",
      "normalizeText: 0\n",
      "dropoutLHS: 0\n",
      "dropoutRHS: 0\n",
      "useWeight: 0\n",
      "weightSep: :\n",
      "Start to initialize starspace model.\n",
      "Build dict from input file : data/prepared_train.tsv\n",
      "Read 12M words\n",
      "Number of words in dictionary:  95058\n",
      "Number of labels in dictionary: 0\n",
      "Loading data from file : data/prepared_train.tsv\n",
      "Total number of examples loaded : 999740\n",
      "Initialized model weights. Model size :\n",
      "matrix : 95058 100\n",
      "Training epoch 0: 0.05 0.01\n",
      "Epoch: 100.0%  lr: 0.040220  loss: 0.042996  eta: 0h10m  tot: 0h2m35s  (20.0%): 0.050000  loss: 0.300170  eta: 0h14m  tot: 0h0m0s  (0.1%)1.5%  lr: 0.049920  loss: 0.192484  eta: 0h11m  tot: 0h0m2s  (0.3%)5.0%  lr: 0.049520  loss: 0.127129  eta: 0h11m  tot: 0h0m7s  (1.0%)5.4%  lr: 0.049489  loss: 0.123146  eta: 0h11m  tot: 0h0m7s  (1.1%)  lr: 0.049459  loss: 0.121018  eta: 0h11m  tot: 0h0m8s  (1.1%)%  lr: 0.049439  loss: 0.120498  eta: 0h11m  tot: 0h0m8s  (1.1%)5.9%  lr: 0.049429  loss: 0.119200  eta: 0h12m  tot: 0h0m8s  (1.2%)%  lr: 0.049279  loss: 0.109414  eta: 0h11m  tot: 0h0m10s  (1.4%)0.049229  loss: 0.106801  eta: 0h11m  tot: 0h0m11s  (1.6%)%  lr: 0.049199  loss: 0.104386  eta: 0h11m  tot: 0h0m11s  (1.6%)%  lr: 0.049169  loss: 0.101940  eta: 0h11m  tot: 0h0m12s  (1.7%)9.2%  lr: 0.049049  loss: 0.099605  eta: 0h11m  tot: 0h0m13s  (1.8%)9.5%  lr: 0.049019  loss: 0.097577  eta: 0h11m  tot: 0h0m13s  (1.9%)%  lr: 0.048939  loss: 0.095247  eta: 0h11m  tot: 0h0m14s  (2.1%)12.7%  lr: 0.048739  loss: 0.090218  eta: 0h11m  tot: 0h0m17s  (2.5%)12.8%  lr: 0.048729  loss: 0.089946  eta: 0h11m  tot: 0h0m17s  (2.6%)13.0%  lr: 0.048709  loss: 0.089795  eta: 0h11m  tot: 0h0m18s  (2.6%)%  lr: 0.048679  loss: 0.089329  eta: 0h11m  tot: 0h0m18s  (2.6%)13.8%  lr: 0.048609  loss: 0.087840  eta: 0h11m  tot: 0h0m19s  (2.8%)14.0%  lr: 0.048579  loss: 0.087343  eta: 0h11m  tot: 0h0m19s  (2.8%)16.5%  lr: 0.048328  loss: 0.082296  eta: 0h10m  tot: 0h0m22s  (3.3%)18.5%  lr: 0.048168  loss: 0.078497  eta: 0h10m  tot: 0h0m24s  (3.7%)19.4%  lr: 0.048038  loss: 0.077226  eta: 0h10m  tot: 0h0m26s  (3.9%)20.7%  lr: 0.047998  loss: 0.075540  eta: 0h10m  tot: 0h0m27s  (4.1%)  lr: 0.047958  loss: 0.074639  eta: 0h10m  tot: 0h0m28s  (4.2%)21.9%  lr: 0.047888  loss: 0.073915  eta: 0h10m  tot: 0h0m29s  (4.4%)22.7%  lr: 0.047788  loss: 0.073100  eta: 0h10m  tot: 0h0m30s  (4.5%)23.5%  lr: 0.047728  loss: 0.072238  eta: 0h10m  tot: 0h0m31s  (4.7%)23.7%  lr: 0.047718  loss: 0.072060  eta: 0h10m  tot: 0h0m32s  (4.7%)24.5%  lr: 0.047648  loss: 0.071121  eta: 0h10m  tot: 0h0m33s  (4.9%)26.5%  lr: 0.047468  loss: 0.069081  eta: 0h10m  tot: 0h0m36s  (5.3%)26.7%  lr: 0.047447  loss: 0.068670  eta: 0h10m  tot: 0h0m36s  (5.3%)28.0%  lr: 0.047317  loss: 0.067357  eta: 0h10m  tot: 0h0m38s  (5.6%)31.1%  lr: 0.047087  loss: 0.065179  eta: 0h10m  tot: 0h0m42s  (6.2%)31.4%  lr: 0.047047  loss: 0.064802  eta: 0h10m  tot: 0h0m42s  (6.3%)32.0%  lr: 0.046997  loss: 0.064414  eta: 0h10m  tot: 0h0m43s  (6.4%)33.3%  lr: 0.046937  loss: 0.063672  eta: 0h10m  tot: 0h0m45s  (6.7%)%  lr: 0.046817  loss: 0.062630  eta: 0h10m  tot: 0h0m47s  (7.0%)35.4%  lr: 0.046707  loss: 0.062295  eta: 0h10m  tot: 0h0m48s  (7.1%)36.2%  lr: 0.046607  loss: 0.061725  eta: 0h10m  tot: 0h0m49s  (7.2%)%  lr: 0.046396  loss: 0.060547  eta: 0h10m  tot: 0h0m52s  (7.6%)38.3%  lr: 0.046396  loss: 0.060538  eta: 0h10m  tot: 0h0m52s  (7.7%)39.7%  lr: 0.046266  loss: 0.059799  eta: 0h10m  tot: 0h0m54s  (7.9%)%  lr: 0.046196  loss: 0.059383  eta: 0h10m  tot: 0h0m55s  (8.1%)40.7%  lr: 0.046166  loss: 0.059300  eta: 0h10m  tot: 0h0m56s  (8.1%)41.0%  lr: 0.046116  loss: 0.059128  eta: 0h10m  tot: 0h0m56s  (8.2%)41.0%  lr: 0.046106  loss: 0.059042  eta: 0h10m  tot: 0h0m56s  (8.2%)41.3%  lr: 0.046096  loss: 0.058842  eta: 0h10m  tot: 0h0m56s  (8.3%)42.0%  lr: 0.046036  loss: 0.058496  eta: 0h10m  tot: 0h0m57s  (8.4%)42.4%  lr: 0.046006  loss: 0.058258  eta: 0h10m  tot: 0h0m58s  (8.5%)42.8%  lr: 0.045966  loss: 0.058068  eta: 0h10m  tot: 0h0m58s  (8.6%)43.6%  lr: 0.045906  loss: 0.057815  eta: 0h10m  tot: 0h1m0s  (8.7%)43.8%  lr: 0.045846  loss: 0.057693  eta: 0h10m  tot: 0h1m0s  (8.8%)44.8%  lr: 0.045746  loss: 0.057200  eta: 0h10m  tot: 0h1m2s  (9.0%)%  lr: 0.045656  loss: 0.056755  eta: 0h10m  tot: 0h1m3s  (9.1%)46.1%  lr: 0.045616  loss: 0.056576  eta: 0h10m  tot: 0h1m4s  (9.2%)%  lr: 0.045586  loss: 0.056346  eta: 0h10m  tot: 0h1m4s  (9.3%)  lr: 0.045496  loss: 0.056009  eta: 0h10m  tot: 0h1m6s  (9.5%)48.0%  lr: 0.045436  loss: 0.055788  eta: 0h10m  tot: 0h1m7s  (9.6%)0h10m  tot: 0h1m7s  (9.7%)48.4%  lr: 0.045385  loss: 0.055639  eta: 0h10m  tot: 0h1m7s  (9.7%)48.8%  lr: 0.045325  loss: 0.055546  eta: 0h10m  tot: 0h1m8s  (9.8%)49.0%  lr: 0.045295  loss: 0.055416  eta: 0h10m  tot: 0h1m8s  (9.8%)49.1%  lr: 0.045295  loss: 0.055362  eta: 0h10m  tot: 0h1m8s  (9.8%)%  lr: 0.045195  loss: 0.055013  eta: 0h10m  tot: 0h1m10s  (10.0%)51.0%  lr: 0.045095  loss: 0.054645  eta: 0h10m  tot: 0h1m12s  (10.2%)51.4%  lr: 0.045045  loss: 0.054496  eta: 0h10m  tot: 0h1m12s  (10.3%)52.1%  lr: 0.045015  loss: 0.054283  eta: 0h10m  tot: 0h1m13s  (10.4%)53.3%  lr: 0.044915  loss: 0.053907  eta: 0h10m  tot: 0h1m15s  (10.7%)54.4%  lr: 0.044805  loss: 0.053459  eta: 0h10m  tot: 0h1m17s  (10.9%)55.0%  lr: 0.044755  loss: 0.053294  eta: 0h10m  tot: 0h1m18s  (11.0%)56.2%  lr: 0.044635  loss: 0.052955  eta: 0h10m  tot: 0h1m20s  (11.2%)57.7%  lr: 0.044525  loss: 0.052575  eta: 0h10m  tot: 0h1m22s  (11.5%)57.8%  lr: 0.044525  loss: 0.052548  eta: 0h10m  tot: 0h1m22s  (11.6%)1m23s  (11.7%)59.3%  lr: 0.044404  loss: 0.052028  eta: 0h10m  tot: 0h1m24s  (11.9%)61.8%  lr: 0.044074  loss: 0.051139  eta: 0h10m  tot: 0h1m28s  (12.4%)64.5%  lr: 0.043834  loss: 0.050351  eta: 0h10m  tot: 0h1m33s  (12.9%)%  lr: 0.043684  loss: 0.050009  eta: 0h10m  tot: 0h1m36s  (13.2%)%  lr: 0.043464  loss: 0.049292  eta: 0h10m  tot: 0h1m39s  (13.6%)68.4%  lr: 0.043434  loss: 0.049243  eta: 0h10m  tot: 0h1m40s  (13.7%)68.6%  lr: 0.043404  loss: 0.049162  eta: 0h10m  tot: 0h1m40s  (13.7%)69.0%  lr: 0.043363  loss: 0.049004  eta: 0h10m  tot: 0h1m41s  (13.8%)71.4%  lr: 0.043123  loss: 0.048514  eta: 0h10m  tot: 0h1m45s  (14.3%)75.3%  lr: 0.042773  loss: 0.047565  eta: 0h10m  tot: 0h1m52s  (15.1%)%  lr: 0.042723  loss: 0.047349  eta: 0h10m  tot: 0h1m54s  (15.2%)  lr: 0.042563  loss: 0.047082  eta: 0h10m  tot: 0h1m56s  (15.4%)%  lr: 0.042563  loss: 0.047037  eta: 0h10m  tot: 0h1m56s  (15.5%)78.4%  lr: 0.042533  loss: 0.046873  eta: 0h10m  tot: 0h1m58s  (15.7%)79.4%  lr: 0.042443  loss: 0.046648  eta: 0h10m  tot: 0h1m59s  (15.9%)80.0%  lr: 0.042383  loss: 0.046569  eta: 0h10m  tot: 0h2m1s  (16.0%)%)83.0%  lr: 0.042092  loss: 0.046071  eta: 0h10m  tot: 0h2m6s  (16.6%)83.3%  lr: 0.042062  loss: 0.046043  eta: 0h10m  tot: 0h2m6s  (16.7%)83.5%  lr: 0.042032  loss: 0.045987  eta: 0h10m  tot: 0h2m7s  (16.7%)0.042002  loss: 0.045926  eta: 0h10m  tot: 0h2m7s  (16.7%)83.8%  lr: 0.041992  loss: 0.045913  eta: 0h10m  tot: 0h2m7s  (16.8%)85.6%  lr: 0.041812  loss: 0.045539  eta: 0h10m  tot: 0h2m10s  (17.1%)86.4%  lr: 0.041692  loss: 0.045369  eta: 0h10m  tot: 0h2m12s  (17.3%)m12s  (17.3%)2m15s  (17.6%)88.1%  lr: 0.041472  loss: 0.044999  eta: 0h10m  tot: 0h2m15s  (17.6%)91.1%  lr: 0.041071  loss: 0.044572  eta: 0h10m  tot: 0h2m20s  (18.2%)92.9%  lr: 0.040851  loss: 0.044328  eta: 0h10m  tot: 0h2m24s  (18.6%)0h10m  tot: 0h2m26s  (18.8%)h2m26s  (18.9%)94.9%  lr: 0.040611  loss: 0.043911  eta: 0h10m  tot: 0h2m27s  (19.0%)96.3%  lr: 0.040481  loss: 0.043625  eta: 0h10m  tot: 0h2m29s  (19.3%)97.5%  lr: 0.040391  loss: 0.043447  eta: 0h10m  tot: 0h2m31s  (19.5%)%  lr: 0.040371  loss: 0.043387  eta: 0h10m  tot: 0h2m32s  (19.6%)98.0%  lr: 0.040361  loss: 0.043325  eta: 0h10m  tot: 0h2m32s  (19.6%)\n",
      " ---+++                Epoch    0 Train error : 0.04345512 +++--- ☃\n",
      "Training epoch 1: 0.04 0.01\n",
      "Epoch: 100.0%  lr: 0.030000  loss: 0.013439  eta: 0h8m  tot: 0h5m33s  (40.0%) lr: 0.040000  loss: 0.010028  eta: 0h10m  tot: 0h2m41s  (20.1%)0.8%  lr: 0.039940  loss: 0.013419  eta: 0h10m  tot: 0h2m42s  (20.2%)1.7%  lr: 0.039840  loss: 0.012875  eta: 0h11m  tot: 0h2m43s  (20.3%)3.1%  lr: 0.039660  loss: 0.013111  eta: 0h10m  tot: 0h2m46s  (20.6%)  lr: 0.039640  loss: 0.013043  eta: 0h10m  tot: 0h2m46s  (20.7%)%  lr: 0.039500  loss: 0.012523  eta: 0h11m  tot: 0h2m49s  (21.0%)5.8%  lr: 0.039409  loss: 0.012896  eta: 0h11m  tot: 0h2m50s  (21.2%)6.8%  lr: 0.039309  loss: 0.012720  eta: 0h11m  tot: 0h2m52s  (21.4%)9.3%  lr: 0.039009  loss: 0.013322  eta: 0h11m  tot: 0h2m57s  (21.9%)10.3%  lr: 0.038909  loss: 0.013550  eta: 0h12m  tot: 0h2m59s  (22.1%)  eta: 0h11m  tot: 0h3m3s  (22.4%)  lr: 0.038448  loss: 0.013244  eta: 0h11m  tot: 0h3m8s  (23.0%)15.3%  lr: 0.038428  loss: 0.013199  eta: 0h11m  tot: 0h3m8s  (23.1%)15.6%  lr: 0.038388  loss: 0.013229  eta: 0h11m  tot: 0h3m9s  (23.1%)  tot: 0h3m11s  (23.3%)17.4%  lr: 0.038238  loss: 0.013084  eta: 0h11m  tot: 0h3m12s  (23.5%)18.2%  lr: 0.038138  loss: 0.012986  eta: 0h11m  tot: 0h3m14s  (23.6%)21.1%  lr: 0.037858  loss: 0.013040  eta: 0h11m  tot: 0h3m19s  (24.2%)22.6%  lr: 0.037688  loss: 0.012993  eta: 0h11m  tot: 0h3m22s  (24.5%)22.7%  lr: 0.037668  loss: 0.013008  eta: 0h11m  tot: 0h3m22s  (24.5%)23.2%  lr: 0.037598  loss: 0.012958  eta: 0h11m  tot: 0h3m23s  (24.6%)%  lr: 0.037528  loss: 0.013016  eta: 0h11m  tot: 0h3m24s  (24.8%)h3m29s  (25.4%)29.3%  lr: 0.036937  loss: 0.013230  eta: 0h11m  tot: 0h3m34s  (25.9%)%  lr: 0.036597  loss: 0.013409  eta: 0h11m  tot: 0h3m40s  (26.6%)34.3%  lr: 0.036497  loss: 0.013469  eta: 0h10m  tot: 0h3m42s  (26.9%)34.7%  lr: 0.036477  loss: 0.013466  eta: 0h10m  tot: 0h3m43s  (26.9%)37.2%  lr: 0.036276  loss: 0.013544  eta: 0h10m  tot: 0h3m46s  (27.4%)0.036176  loss: 0.013523  eta: 0h10m  tot: 0h3m47s  (27.5%)  loss: 0.013468  eta: 0h10m  tot: 0h3m50s  (27.8%)39.2%  lr: 0.035996  loss: 0.013516  eta: 0h10m  tot: 0h3m50s  (27.8%)40.2%  lr: 0.035926  loss: 0.013479  eta: 0h10m  tot: 0h3m51s  (28.0%)40.5%  lr: 0.035906  loss: 0.013482  eta: 0h10m  tot: 0h3m52s  (28.1%)42.1%  lr: 0.035696  loss: 0.013405  eta: 0h10m  tot: 0h3m55s  (28.4%)43.2%  lr: 0.035576  loss: 0.013400  eta: 0h10m  tot: 0h3m57s  (28.6%)29.0%)0.013450  eta: 0h10m  tot: 0h4m0s  (29.0%)47.4%  lr: 0.035175  loss: 0.013587  eta: 0h10m  tot: 0h4m4s  (29.5%)49.3%  lr: 0.034985  loss: 0.013525  eta: 0h10m  tot: 0h4m8s  (29.9%)51.0%  lr: 0.034795  loss: 0.013605  eta: 0h10m  tot: 0h4m10s  (30.2%)52.2%  lr: 0.034705  loss: 0.013649  eta: 0h10m  tot: 0h4m12s  (30.4%)54.8%  lr: 0.034515  loss: 0.013651  eta: 0h10m  tot: 0h4m17s  (31.0%)  lr: 0.034234  loss: 0.013615  eta: 0h10m  tot: 0h4m21s  (31.5%)%  lr: 0.034114  loss: 0.013541  eta: 0h9m  tot: 0h4m23s  (31.8%)%  lr: 0.034104  loss: 0.013565  eta: 0h9m  tot: 0h4m23s  (31.8%)59.6%  lr: 0.034044  loss: 0.013542  eta: 0h9m  tot: 0h4m25s  (31.9%)4m26s  (32.0%)61.7%  lr: 0.033824  loss: 0.013586  eta: 0h9m  tot: 0h4m29s  (32.3%)63.4%  lr: 0.033624  loss: 0.013631  eta: 0h9m  tot: 0h4m32s  (32.7%)63.5%  lr: 0.033624  loss: 0.013619  eta: 0h9m  tot: 0h4m33s  (32.7%)63.6%  lr: 0.033594  loss: 0.013618  eta: 0h9m  tot: 0h4m33s  (32.7%)68.0%  lr: 0.033093  loss: 0.013604  eta: 0h9m  tot: 0h4m40s  (33.6%)68.1%  lr: 0.033093  loss: 0.013597  eta: 0h9m  tot: 0h4m40s  (33.6%)72.3%  lr: 0.032623  loss: 0.013569  eta: 0h9m  tot: 0h4m47s  (34.5%)75.0%  lr: 0.032362  loss: 0.013581  eta: 0h9m  tot: 0h4m51s  (35.0%)75.3%  lr: 0.032332  loss: 0.013563  eta: 0h9m  tot: 0h4m52s  (35.1%)78.2%  lr: 0.032052  loss: 0.013572  eta: 0h9m  tot: 0h4m57s  (35.6%)79.1%  lr: 0.031992  loss: 0.013584  eta: 0h9m  tot: 0h4m58s  (35.8%)79.9%  lr: 0.031902  loss: 0.013545  eta: 0h9m  tot: 0h4m59s  (36.0%)81.2%  lr: 0.031782  loss: 0.013557  eta: 0h9m  tot: 0h5m2s  (36.2%)%  lr: 0.031732  loss: 0.013559  eta: 0h9m  tot: 0h5m2s  (36.3%)81.6%  lr: 0.031732  loss: 0.013561  eta: 0h9m  tot: 0h5m2s  (36.3%)82.7%  lr: 0.031592  loss: 0.013551  eta: 0h9m  tot: 0h5m4s  (36.5%)m  tot: 0h5m5s  (36.6%)86.7%  lr: 0.031191  loss: 0.013584  eta: 0h9m  tot: 0h5m11s  (37.3%)  lr: 0.030941  loss: 0.013507  eta: 0h8m  tot: 0h5m16s  (37.9%)93.6%  lr: 0.030481  loss: 0.013502  eta: 0h8m  tot: 0h5m23s  (38.7%)\n",
      " ---+++                Epoch    1 Train error : 0.01329501 +++--- ☃\n",
      "Training epoch 2: 0.03 0.01\n",
      "Epoch: 100.0%  lr: 0.020040  loss: 0.009309  eta: 0h5m  tot: 0h8m26s  (60.0%)009094  eta: 0h8m  tot: 0h5m33s  (40.1%)  lr: 0.029930  loss: 0.008438  eta: 0h7m  tot: 0h5m35s  (40.3%)0h7m  tot: 0h5m36s  (40.5%)3.7%  lr: 0.029800  loss: 0.008876  eta: 0h7m  tot: 0h5m39s  (40.7%)4.2%  lr: 0.029780  loss: 0.008984  eta: 0h7m  tot: 0h5m40s  (40.8%)41.0%)%  lr: 0.029650  loss: 0.009390  eta: 0h7m  tot: 0h5m41s  (41.1%)5.6%  lr: 0.029630  loss: 0.009405  eta: 0h8m  tot: 0h5m42s  (41.1%)6.0%  lr: 0.029600  loss: 0.009297  eta: 0h7m  tot: 0h5m43s  (41.2%)41.4%)7.8%  lr: 0.029439  loss: 0.009177  eta: 0h8m  tot: 0h5m46s  (41.6%)8.3%  lr: 0.029399  loss: 0.009214  eta: 0h8m  tot: 0h5m47s  (41.7%)8.8%  lr: 0.029359  loss: 0.009349  eta: 0h8m  tot: 0h5m48s  (41.8%)%  lr: 0.029209  loss: 0.009263  eta: 0h8m  tot: 0h5m50s  (42.0%)11.2%  lr: 0.029149  loss: 0.009052  eta: 0h8m  tot: 0h5m52s  (42.2%)13.5%  lr: 0.028949  loss: 0.009080  eta: 0h8m  tot: 0h5m56s  (42.7%)  lr: 0.028859  loss: 0.009055  eta: 0h8m  tot: 0h5m57s  (42.8%)15.8%  lr: 0.028679  loss: 0.008926  eta: 0h8m  tot: 0h6m0s  (43.2%)16.4%  lr: 0.028629  loss: 0.008980  eta: 0h8m  tot: 0h6m1s  (43.3%)m  tot: 0h6m3s  (43.5%)17.6%  lr: 0.028519  loss: 0.009077  eta: 0h8m  tot: 0h6m3s  (43.5%)45.5%)  eta: 0h7m  tot: 0h6m24s  (45.9%)30.2%  lr: 0.027077  loss: 0.009295  eta: 0h7m  tot: 0h6m25s  (46.0%)m29s  (46.4%)46.5%)36.9%  lr: 0.026547  loss: 0.009094  eta: 0h7m  tot: 0h6m37s  (47.4%)43.0%  lr: 0.025836  loss: 0.009221  eta: 0h7m  tot: 0h6m49s  (48.6%)45.1%  lr: 0.025536  loss: 0.009194  eta: 0h7m  tot: 0h6m53s  (49.0%)0h7m  tot: 0h6m54s  (49.1%)7m  tot: 0h6m57s  (49.4%)47.3%  lr: 0.025375  loss: 0.009248  eta: 0h7m  tot: 0h6m57s  (49.5%)48.3%  lr: 0.025235  loss: 0.009277  eta: 0h7m  tot: 0h7m0s  (49.7%)48.8%  lr: 0.025235  loss: 0.009265  eta: 0h7m  tot: 0h7m0s  (49.8%)%  lr: 0.024915  loss: 0.009180  eta: 0h7m  tot: 0h7m5s  (50.4%)%  lr: 0.024905  loss: 0.009173  eta: 0h7m  tot: 0h7m6s  (50.4%)52.8%  lr: 0.024795  loss: 0.009160  eta: 0h7m  tot: 0h7m7s  (50.6%)55.0%  lr: 0.024635  loss: 0.009165  eta: 0h7m  tot: 0h7m11s  (51.0%)  lr: 0.024595  loss: 0.009174  eta: 0h7m  tot: 0h7m11s  (51.1%)55.6%  lr: 0.024575  loss: 0.009196  eta: 0h7m  tot: 0h7m12s  (51.1%)  tot: 0h7m16s  (51.6%)59.5%  lr: 0.024264  loss: 0.009131  eta: 0h7m  tot: 0h7m18s  (51.9%)0.024104  loss: 0.009190  eta: 0h7m  tot: 0h7m20s  (52.1%)0.009193  eta: 0h7m  tot: 0h7m21s  (52.1%)61.4%  lr: 0.023994  loss: 0.009219  eta: 0h7m  tot: 0h7m22s  (52.3%)65.5%  lr: 0.023554  loss: 0.009197  eta: 0h6m  tot: 0h7m29s  (53.1%)  tot: 0h7m41s  (54.5%)73.3%  lr: 0.022843  loss: 0.009260  eta: 0h6m  tot: 0h7m42s  (54.7%)73.4%  lr: 0.022823  loss: 0.009253  eta: 0h6m  tot: 0h7m42s  (54.7%)73.8%  lr: 0.022783  loss: 0.009251  eta: 0h6m  tot: 0h7m43s  (54.8%)76.1%  lr: 0.022543  loss: 0.009252  eta: 0h6m  tot: 0h7m47s  (55.2%)78.1%  lr: 0.022413  loss: 0.009294  eta: 0h6m  tot: 0h7m50s  (55.6%)0.009291  eta: 0h6m  tot: 0h7m50s  (55.6%)0.009274  eta: 0h6m  tot: 0h7m53s  (56.1%)0.022132  loss: 0.009275  eta: 0h6m  tot: 0h7m54s  (56.2%)82.6%  lr: 0.021912  loss: 0.009309  eta: 0h6m  tot: 0h7m57s  (56.5%)83.1%  lr: 0.021862  loss: 0.009334  eta: 0h6m  tot: 0h7m58s  (56.6%)87.1%  lr: 0.021452  loss: 0.009312  eta: 0h6m  tot: 0h8m5s  (57.4%)m  tot: 0h8m9s  (57.9%)8m11s  (58.2%)92.5%  lr: 0.020871  loss: 0.009284  eta: 0h6m  tot: 0h8m13s  (58.5%)%  lr: 0.020320  loss: 0.009309  eta: 0h5m  tot: 0h8m22s  (59.4%)98.4%  lr: 0.020120  loss: 0.009296  eta: 0h5m  tot: 0h8m24s  (59.7%)8m25s  (59.8%)99.7%  lr: 0.020050  loss: 0.009316  eta: 0h5m  tot: 0h8m26s  (59.9%)\n",
      " ---+++                Epoch    2 Train error : 0.00946834 +++--- ☃\n",
      "Training epoch 3: 0.02 0.01\n",
      "Epoch: 100.0%  lr: 0.010000  loss: 0.007717  eta: 0h3m  tot: 0h11m30s  (80.0%)%  lr: 0.019950  loss: 0.010825  eta: 0h3m  tot: 0h8m28s  (60.1%)%)%  lr: 0.019640  loss: 0.008087  eta: 0h5m  tot: 0h8m34s  (60.7%)4.7%  lr: 0.019479  loss: 0.008048  eta: 0h6m  tot: 0h8m37s  (60.9%)8.0%  lr: 0.019149  loss: 0.007739  eta: 0h5m  tot: 0h8m42s  (61.6%)9.2%  lr: 0.019019  loss: 0.007728  eta: 0h5m  tot: 0h8m44s  (61.8%)10.4%  lr: 0.018949  loss: 0.007548  eta: 0h5m  tot: 0h8m46s  (62.1%)11.0%  lr: 0.018909  loss: 0.007529  eta: 0h5m  tot: 0h8m47s  (62.2%)11.2%  lr: 0.018869  loss: 0.007509  eta: 0h5m  tot: 0h8m47s  (62.2%)12.9%  lr: 0.018739  loss: 0.007676  eta: 0h5m  tot: 0h8m50s  (62.6%)13.5%  lr: 0.018689  loss: 0.007621  eta: 0h5m  tot: 0h8m51s  (62.7%)  eta: 0h5m  tot: 0h8m53s  (62.9%)18.1%  lr: 0.018138  loss: 0.007451  eta: 0h5m  tot: 0h9m0s  (63.6%)18.5%  lr: 0.018068  loss: 0.007473  eta: 0h5m  tot: 0h9m1s  (63.7%)18.6%  lr: 0.018068  loss: 0.007453  eta: 0h5m  tot: 0h9m1s  (63.7%)20.7%  lr: 0.017818  loss: 0.007346  eta: 0h5m  tot: 0h9m5s  (64.1%)0.007319  eta: 0h5m  tot: 0h9m5s  (64.2%)22.2%  lr: 0.017778  loss: 0.007273  eta: 0h5m  tot: 0h9m8s  (64.4%)22.5%  lr: 0.017768  loss: 0.007282  eta: 0h5m  tot: 0h9m8s  (64.5%)22.6%  lr: 0.017768  loss: 0.007325  eta: 0h5m  tot: 0h9m8s  (64.5%)0.017678  loss: 0.007330  eta: 0h5m  tot: 0h9m10s  (64.8%)25.4%  lr: 0.017417  loss: 0.007303  eta: 0h5m  tot: 0h9m14s  (65.1%)9m15s  (65.2%)  eta: 0h5m  tot: 0h9m28s  (66.5%)35.2%  lr: 0.016517  loss: 0.007430  eta: 0h5m  tot: 0h9m33s  (67.0%)37.6%  lr: 0.016256  loss: 0.007459  eta: 0h5m  tot: 0h9m38s  (67.5%)  lr: 0.015976  loss: 0.007534  eta: 0h4m  tot: 0h9m44s  (68.1%)40.8%  lr: 0.015966  loss: 0.007535  eta: 0h4m  tot: 0h9m44s  (68.2%)0.007535  eta: 0h4m  tot: 0h9m45s  (68.2%)m45s  (68.2%)41.4%  lr: 0.015876  loss: 0.007526  eta: 0h4m  tot: 0h9m45s  (68.3%)42.9%  lr: 0.015736  loss: 0.007525  eta: 0h4m  tot: 0h9m48s  (68.6%)45.3%  lr: 0.015456  loss: 0.007453  eta: 0h4m  tot: 0h9m52s  (69.1%)m  tot: 0h9m54s  (69.3%)%  lr: 0.015235  loss: 0.007560  eta: 0h4m  tot: 0h9m58s  (69.7%)4m  tot: 0h10m1s  (70.1%)51.0%  lr: 0.015055  loss: 0.007499  eta: 0h4m  tot: 0h10m2s  (70.2%)%  lr: 0.014875  loss: 0.007475  eta: 0h4m  tot: 0h10m5s  (70.5%)53.1%  lr: 0.014805  loss: 0.007504  eta: 0h4m  tot: 0h10m7s  (70.6%)%  lr: 0.014274  loss: 0.007465  eta: 0h4m  tot: 0h10m16s  (71.6%)58.6%  lr: 0.014254  loss: 0.007492  eta: 0h4m  tot: 0h10m17s  (71.7%)64.7%  lr: 0.013534  loss: 0.007531  eta: 0h4m  tot: 0h10m27s  (72.9%) (73.3%)  tot: 0h10m32s  (73.4%)0.013253  loss: 0.007604  eta: 0h4m  tot: 0h10m32s  (73.4%)m  tot: 0h10m37s  (73.9%)69.4%  lr: 0.013063  loss: 0.007613  eta: 0h4m  tot: 0h10m37s  (73.9%)0.013053  loss: 0.007613  eta: 0h4m  tot: 0h10m37s  (73.9%)73.4%  lr: 0.012653  loss: 0.007623  eta: 0h3m  tot: 0h10m43s  (74.7%)  eta: 0h3m  tot: 0h10m49s  (75.2%)78.7%  lr: 0.012112  loss: 0.007627  eta: 0h3m  tot: 0h10m52s  (75.7%)%  lr: 0.012072  loss: 0.007634  eta: 0h3m  tot: 0h10m53s  (75.8%)79.6%  lr: 0.012022  loss: 0.007632  eta: 0h3m  tot: 0h10m54s  (75.9%)%  lr: 0.012012  loss: 0.007641  eta: 0h3m  tot: 0h10m54s  (75.9%)81.4%  lr: 0.011892  loss: 0.007651  eta: 0h3m  tot: 0h10m57s  (76.3%)82.1%  lr: 0.011822  loss: 0.007629  eta: 0h3m  tot: 0h10m58s  (76.4%)m  tot: 0h11m0s  (76.6%)84.3%  lr: 0.011532  loss: 0.007654  eta: 0h3m  tot: 0h11m2s  (76.9%)84.9%  lr: 0.011502  loss: 0.007649  eta: 0h3m  tot: 0h11m3s  (77.0%)85.7%  lr: 0.011382  loss: 0.007657  eta: 0h3m  tot: 0h11m5s  (77.1%)86.9%  lr: 0.011291  loss: 0.007650  eta: 0h3m  tot: 0h11m8s  (77.4%)88.1%  lr: 0.011111  loss: 0.007651  eta: 0h3m  tot: 0h11m10s  (77.6%)89.2%  lr: 0.011031  loss: 0.007640  eta: 0h3m  tot: 0h11m12s  (77.8%)  loss: 0.007638  eta: 0h3m  tot: 0h11m14s  (78.1%)  eta: 0h3m  tot: 0h11m21s  (78.8%)  tot: 0h11m21s  (78.8%)94.8%  lr: 0.010461  loss: 0.007709  eta: 0h3m  tot: 0h11m22s  (79.0%)95.3%  lr: 0.010391  loss: 0.007710  eta: 0h3m  tot: 0h11m23s  (79.1%)\n",
      " ---+++                Epoch    3 Train error : 0.00778375 +++--- ☃\n",
      "Training epoch 4: 0.01 0.01\n",
      "Epoch: 100.0%  lr: 0.000020  loss: 0.006796  eta: <1min   tot: 0h14m31s  (100.0%) lr: 0.009970  loss: 0.006863  eta: 0h2m  tot: 0h11m31s  (80.1%)  eta: 0h2m  tot: 0h11m31s  (80.1%)1.8%  lr: 0.009900  loss: 0.007363  eta: 0h2m  tot: 0h11m32s  (80.4%)2.2%  lr: 0.009870  loss: 0.007679  eta: 0h2m  tot: 0h11m33s  (80.4%)%  lr: 0.009760  loss: 0.007661  eta: 0h2m  tot: 0h11m35s  (80.6%)8.1%  lr: 0.009279  loss: 0.007682  eta: 0h2m  tot: 0h11m44s  (81.6%)9.4%  lr: 0.009179  loss: 0.007460  eta: 0h2m  tot: 0h11m46s  (81.9%)0.008969  loss: 0.007222  eta: 0h2m  tot: 0h11m51s  (82.3%)19.7%  lr: 0.008158  loss: 0.007090  eta: 0h2m  tot: 0h12m5s  (83.9%)30.2%  lr: 0.007147  loss: 0.006918  eta: 0h2m  tot: 0h12m23s  (86.0%)30.6%  lr: 0.007067  loss: 0.006903  eta: 0h2m  tot: 0h12m24s  (86.1%)2m  tot: 0h12m28s  (86.5%)33.2%  lr: 0.006807  loss: 0.006939  eta: 0h1m  tot: 0h12m29s  (86.6%)0.006907  eta: 0h1m  tot: 0h12m30s  (86.8%)34.1%  lr: 0.006747  loss: 0.006894  eta: 0h1m  tot: 0h12m31s  (86.8%)  eta: 0h1m  tot: 0h12m32s  (87.0%)37.2%  lr: 0.006426  loss: 0.006868  eta: 0h1m  tot: 0h12m36s  (87.4%)38.1%  lr: 0.006336  loss: 0.006835  eta: 0h1m  tot: 0h12m38s  (87.6%)40.1%  lr: 0.006116  loss: 0.006875  eta: 0h1m  tot: 0h12m42s  (88.0%)%  lr: 0.006066  loss: 0.006859  eta: 0h1m  tot: 0h12m43s  (88.2%)41.2%  lr: 0.006046  loss: 0.006862  eta: 0h1m  tot: 0h12m44s  (88.2%)41.5%  lr: 0.006016  loss: 0.006856  eta: 0h1m  tot: 0h12m45s  (88.3%)  tot: 0h12m45s  (88.3%)0.006904  eta: 0h1m  tot: 0h12m48s  (88.7%)47.4%  lr: 0.005305  loss: 0.006850  eta: 0h1m  tot: 0h12m56s  (89.5%)50.0%  lr: 0.005055  loss: 0.006871  eta: 0h1m  tot: 0h13m0s  (90.0%)50.1%  lr: 0.005045  loss: 0.006894  eta: 0h1m  tot: 0h13m0s  (90.0%)51.0%  lr: 0.004965  loss: 0.006874  eta: 0h1m  tot: 0h13m2s  (90.2%)51.1%  lr: 0.004955  loss: 0.006876  eta: 0h1m  tot: 0h13m2s  (90.2%)0.004675  loss: 0.006926  eta: 0h1m  tot: 0h13m6s  (90.7%)54.0%  lr: 0.004575  loss: 0.006921  eta: 0h1m  tot: 0h13m7s  (90.8%)%  lr: 0.004435  loss: 0.006885  eta: 0h1m  tot: 0h13m11s  (91.2%)57.2%  lr: 0.004324  loss: 0.006897  eta: 0h1m  tot: 0h13m13s  (91.4%)58.0%  lr: 0.004194  loss: 0.006906  eta: 0h1m  tot: 0h13m14s  (91.6%)13m21s  (92.4%)%  lr: 0.003143  loss: 0.006906  eta: <1min   tot: 0h13m32s  (93.6%)68.2%  lr: 0.003073  loss: 0.006901  eta: <1min   tot: 0h13m33s  (93.6%)m33s  (93.7%)68.6%  lr: 0.003043  loss: 0.006895  eta: <1min   tot: 0h13m34s  (93.7%)94.0%)%  lr: 0.002783  loss: 0.006920  eta: <1min   tot: 0h13m39s  (94.3%)72.3%  lr: 0.002703  loss: 0.006916  eta: <1min   tot: 0h13m40s  (94.5%)0.006899  eta: <1min   tot: 0h13m43s  (94.8%)  lr: 0.002553  loss: 0.006897  eta: <1min   tot: 0h13m44s  (94.9%)%  lr: 0.002252  loss: 0.006870  eta: <1min   tot: 0h13m51s  (95.6%)79.2%  lr: 0.002152  loss: 0.006854  eta: <1min   tot: 0h13m53s  (95.8%)86.2%  lr: 0.001522  loss: 0.006833  eta: <1min   tot: 0h14m5s  (97.2%)0h14m5s  (97.3%)%  lr: 0.001422  loss: 0.006815  eta: <1min   tot: 0h14m7s  (97.4%)%  lr: 0.001401  loss: 0.006829  eta: <1min   tot: 0h14m7s  (97.5%)%  lr: 0.000300  loss: 0.006799  eta: <1min   tot: 0h14m26s  (99.5%)%  lr: 0.000140  loss: 0.006787  eta: <1min   tot: 0h14m29s  (99.7%)99.9%  lr: 0.000020  loss: 0.006798  eta: <1min   tot: 0h14m31s  (100.0%)\n",
      " ---+++                Epoch    4 Train error : 0.00697917 +++--- ☃\n",
      "Saving model to file : model-ss-tags\n",
      "Saving model in tsv format : model-ss-tags.tsv\n"
     ]
    }
   ],
   "source": [
    "######### TRAINING HAPPENING HERE #############\n",
    "!bash ./train.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can compare the new embeddings with the previous ones. You can find trained word vectors in the file *[model_file_name].tsv*. Upload the embeddings from StarSpace into a dict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # starspace_validation = []\n",
    "starspace_embeddings = {}\n",
    "for line in open('model-ss-tags.tsv'):\n",
    "    line = line.strip().split('\\t')\n",
    "    starspace_embeddings[line[0]] = np.array(list(map(float, line[1:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_prepared_ranking = []\n",
    "for line in prepared_validation:\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, starspace_embeddings, 100)\n",
    "    ss_prepared_ranking.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.482 | Hits@   1: 0.482\n",
      "DCG@   5: 0.588 | Hits@   5: 0.679\n",
      "DCG@  10: 0.608 | Hits@  10: 0.741\n",
      "DCG@ 100: 0.642 | Hits@ 100: 0.904\n",
      "DCG@ 500: 0.652 | Hits@ 500: 0.978\n",
      "DCG@1000: 0.654 | Hits@1000: 1.000\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(ss_prepared_ranking, k), \n",
    "                                               k, hits_count(ss_prepared_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to training for the particular task with the supervised data, you should expect to obtain a higher quality than for the previous approach. In additiion, despite the fact that StarSpace's trained vectors have a smaller dimension than word2vec's, it provides better results in this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5 (StarSpaceRanks).** For each question from prepared *test.tsv* submit the ranks of the candidates for trained representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task StarSpaceRanks is: 99\t22\t87\t68\t20\t89\t47\t98\t33\t30\t17\t35\t14\t4\t83\t95\t1\t7\t34\t73\t66\t92\t32\t70\t80\t21\t86\t96\t48\t3\t61\t72\t56\t90\t52...\n"
     ]
    }
   ],
   "source": [
    "starspace_ranks_results = []\n",
    "prepared_test_data ='data/prepared_test.tsv'\n",
    "for line in open(prepared_test_data):\n",
    "    q, *ex = line.strip().split('\\t')\n",
    "    ranks = rank_candidates(q, ex, starspace_embeddings, 100)\n",
    "    ranked_candidates = [r[0] for r in ranks]\n",
    "    starspace_ranks_results.append([ranked_candidates.index(i) + 1 for i in range(len(ranked_candidates))])\n",
    "    \n",
    "grader.submit_tag('StarSpaceRanks', matrix_to_string(starspace_ranks_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, **don't remove** the file with these embeddings because you will need them in the final project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authorization & Submission\n",
    "To submit assignment parts to Cousera platform, please, enter your e-mail and token into variables below. You can generate token on this programming assignment page. <b>Note:</b> Token expires 30 minutes after generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You want to submit these parts:\n",
      "Task Question2Vec: 0.019293891059027776\n",
      "-0.028727213541666668\n",
      "0.046056111653645836\n",
      "0.08525933159722222\n",
      "0.02430555555555...\n",
      "Task HitsCount: 1.0\n",
      "0.5\n",
      "1.0\n",
      "0.5\n",
      "1.0\n",
      "0.3333333333333333\n",
      "0.6666666666666666\n",
      "1.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1....\n",
      "Task DCGScore: 1.0\n",
      "0.5\n",
      "0.8154648767857288\n",
      "0.5\n",
      "0.8154648767857288\n",
      "0.3333333333333333\n",
      "0.5436432511904858\n",
      "0.7103099178...\n",
      "Task W2VTokenizedRanks: 95\t94\t7\t9\t64\t37\t32\t93\t24\t100\t98\t17\t60\t6\t97\t49\t70\t38\t42\t96\t30\t21\t2\t65\t67\t45\t27\t26\t57\t62\t11\t88\t56\t66\t7...\n",
      "Task StarSpaceRanks: 99\t22\t87\t68\t20\t89\t47\t98\t33\t30\t17\t35\t14\t4\t83\t95\t1\t7\t34\t73\t66\t92\t32\t70\t80\t21\t86\t96\t48\t3\t61\t72\t56\t90\t52...\n"
     ]
    }
   ],
   "source": [
    "STUDENT_EMAIL = \"joao.matos@sidia.com\" \n",
    "STUDENT_TOKEN = 'YUnmrLzKnaVI9Hr8' \n",
    "grader.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to submit these answers, run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted to Coursera platform. See results on assignment page!\n"
     ]
    }
   ],
   "source": [
    "grader.submit(STUDENT_EMAIL, STUDENT_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
